{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "from DSI_Capstone_Steemit.utils.utils import(\n",
    "    load_data_and_description,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "data_directory = '../data/'\n",
    "input_directory = os.path.join(data_directory,'networkx_votes')\n",
    "\n",
    "def load_joblib(filename):\n",
    "    return joblib.load(os.path.join(input_directory,filename))\n",
    "\n",
    "data,feature_names,data_desc = load_data_and_description(data_type='tfidf')\n",
    "data_desc['log total_payout_value'] = np.log(data_desc['total_payout_value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data_desc['depth'] != 0\n",
    "\n",
    "idx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desc = data_desc[idx]\n",
    "data = data[idx.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_desc['total_payout_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_network_features(df):\n",
    "\n",
    "    hubs,authorities = load_joblib('hits')\n",
    "    cluster = load_joblib('parts')\n",
    "    pagerank = load_joblib('prank') \n",
    "    eig_cent = load_joblib('eig_cent') \n",
    "    core_k = load_joblib('core_k') \n",
    "\n",
    "    df['Cluster'] = df['author'].map(cluster)\n",
    "    df.loc[:,'Cluster Condense'] = df['Cluster']\n",
    "    df.loc[~df['Cluster'].isin([1,3,0,2,5,4]),'Cluster Condense'] = 'Other'\n",
    "    df['Hubs'] = df['author'].map(hubs) * 10000\n",
    "    df['Authorities'] = df['author'].map(authorities) * 10000\n",
    "    df['Page Rank'] = df['author'].map(pagerank) * 10000\n",
    "    df['Eigen Centrality'] = df['author'].map(eig_cent)* 10000\n",
    "    df['Core K'] = df['author'].map(core_k)*10000\n",
    "    return df\n",
    "data_desc = add_network_features(data_desc)\n",
    "\n",
    "network_cols = ['Page Rank','Cluster','Hubs','Authorities','Eigen Centrality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove middle value articles\n",
    "\n",
    "# idx1 = data_desc['log total_payout_value'] < 1.2\n",
    "# idx2 = data_desc['log total_payout_value'] >2.5\n",
    "\n",
    "# idx_not = (~idx1) & (~idx2)\n",
    "\n",
    "data_desc = data_desc\n",
    "data = data[:,:]\n",
    "# y = data_desc['log total_payout_value'] >2.5\n",
    "\n",
    "# For Regression\n",
    "y = data_desc['total_payout_value']\n",
    "\n",
    "value_counts = data_desc['category'].value_counts()\n",
    "top_categories = value_counts.index[value_counts > np.percentile(data_desc['category'].value_counts(),97)]\n",
    "idx = data_desc['category'].isin(top_categories)\n",
    "data_desc['top category'] = idx.astype(int)\n",
    "\n",
    "data_desc['top category listed'] = data_desc.ix[data_desc['top category'].values.astype(bool) ,'category']\n",
    "\n",
    "data_desc['top category listed'] = data_desc['top category listed'].fillna('Other')\n",
    "\n",
    "\n",
    "post_features = ['number of body tags',\n",
    "                                   'number of body urls',\n",
    "                                   'number of image urls',\n",
    "                                   'number of body mentions',\n",
    "                                   'number of image urls',\n",
    "                                   'number of youtube urls',\n",
    "                                   'language',\n",
    "                                   'author_reputation_scaled',\n",
    "                                   'number of steem counts',\n",
    "                                'top category'] + network_cols\n",
    "\n",
    "\n",
    "train_features = data_desc.ix[:,post_features].fillna(0)\n",
    "\n",
    "train = pd.get_dummies(train_features)\n",
    "\n",
    "# num_image_urls = train['number of image urls'].values[:,0]\n",
    "# train.drop('number of image urls',axis = 1, inplace=True)\n",
    "\n",
    "train['number of image urls'] = num_image_urls\n",
    "\n",
    "training_names = train.columns\n",
    "\n",
    "train_sparse = ssp.csr_matrix(train)\n",
    "new_data = ssp.hstack([data,train_sparse])\n",
    "train = new_data.tocsr()\n",
    "\n",
    "# All samples\n",
    "number_of_samples = train.shape[0]\n",
    "\n",
    "\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# selector = VarianceThreshold()\n",
    "# print data.shape\n",
    "# train = selector.fit_transform(train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = selector.get_support()\n",
    "\n",
    "\n",
    "all_features = np.array(feature_names + list(training_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_train.shape, len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_train:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha,learning_rate,reg_alpha,reg_lambda,scale_pos_weight):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "    params['learning_rate'] = max(learning_rate, 0)\n",
    "    params['reg_alpha'] = max(reg_alpha, 0)\n",
    "    params['reg_lambda'] = max(reg_lambda, 0)\n",
    "    params['scale_pos_weight'] =  max(scale_pos_weight, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, \n",
    "                       num_boost_round=num_boost_round, nfold=5,\n",
    "             seed=random_state,\n",
    "#              callbacks=[xgb.callback.early_stop(50)]\n",
    "                      )\n",
    "\n",
    "    return -cv_result['test-mae-mean'].mean()\n",
    "\n",
    "\n",
    "num_boost_round = 10\n",
    "\n",
    "random_state = 2016\n",
    "\n",
    "# For Bayesian Optimization\n",
    "num_iter = 30\n",
    "init_points = 5\n",
    "params = {\n",
    "    'eta': 0.05,\n",
    "    'silent': 0,\n",
    "    'eval_metric': ['mae'],\n",
    "    'objective':'reg:linear',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'max_depth': (1, 15),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 10),\n",
    "                                            'reg_lambda': (0, 10),\n",
    "                                            'learning_rate': (0.001,1),\n",
    "                                            'reg_alpha' : (0,2),\n",
    "                                            'alpha':(0,2),\n",
    "                                            'scale_pos_weight':(0,2)\n",
    "                                            })\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print(xgbBO.res['max'])\n",
    "print(xgbBO.res['all'])\n",
    "\n",
    "end = time.time()\n",
    "print 'Final Time'\n",
    "print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbBO.points_to_csv('bayesian_results.csv')\n",
    "# import json\n",
    "# with open('bayesian_results.json', 'w') as fp:\n",
    "#     json.dump(xgbBO.res, fp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline \n",
    "plt.plot(np.array(xgbBO.res['all']['values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_max = xgbBO.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_max.pop('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_max['max_depth'] = int(param_max['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(**xgbBO.res['max']['max_params'])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(predictions,y_test,\n",
    "            scatter_kws={'alpha':0.5}\n",
    "           )\n",
    "\n",
    "print r2_score(y_pred=y_test,y_true=predictions)\n",
    "print mean_absolute_error(y_pred=y_test,y_true=predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame([model.feature_importances_,all_features]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns = ['Importance','Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[features['Importance'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.sort_values('Importance',\n",
    "                                ascending=False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
