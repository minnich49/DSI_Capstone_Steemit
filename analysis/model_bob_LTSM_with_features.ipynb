{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sys\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "data_directory = '../data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "from keras.layers import Embedding, LSTM\n",
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_raw_cleaned = pd.read_csv(os.path.join(data_directory,\n",
    "                                             'posts_raw_cleaned', \n",
    "                                             'posts_raw_cleaned.csv'))\n",
    "\n",
    "# Drop null\n",
    "\n",
    "posts_raw_cleaned = posts_raw_cleaned[posts_raw_cleaned['body'].notnull()]\n",
    "\n",
    "# posts_raw_cleaned = posts_raw_cleaned[0:5000]\n",
    "\n",
    "texts = list(posts_raw_cleaned['body'])\n",
    "\n",
    "labels = posts_raw_cleaned['total_payout_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steem_counts = posts_raw_cleaned['body'].str.lower().str.count('steem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_raw_cleaned['number of steem counts'] = steem_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = []\n",
    "for language in posts_raw_cleaned['body_language']:\n",
    "    if (language != '[]') & pd.notnull(language):\n",
    "        languages.append(json.loads(language)[0]['language'])\n",
    "    else:\n",
    "        languages.append('unknown')\n",
    "        \n",
    "posts_raw_cleaned['language'] = languages        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_raw_cleaned['author_reputation_scaled'] = (posts_raw_cleaned['author_reputation'] + 0.0)/(10**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (posts_raw_cleaned['total_payout_value'] >  np.median(posts_raw_cleaned['total_payout_value'])).astype(int)\n",
    "\n",
    "# labels = (labels > labels.mean()).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NB_WORDS = 5000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to download : http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = os.path.join('..','word2vec_models','glove.6B')\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "labels = ((posts_raw_cleaned['total_payout_value'] >  np.median(posts_raw_cleaned['total_payout_value']))\n",
    "          .astype(int)).values\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.33\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "\n",
    "features = posts_raw_cleaned.ix[:,['number of body tags',\n",
    "                                   'number of body urls',\n",
    "                                   'number of image urls',\n",
    "                                   'number of body mentions',\n",
    "                                   'number of image urls',\n",
    "                                   'number of youtube urls',\n",
    "                                   'language',\n",
    "                                   'author_reputation_scaled',\n",
    "                                   'number of steem counts']]\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "features = ss.fit_transform(features)\n",
    "number_of_features = features.shape[1]\n",
    "x_train_features = features[:-nb_validation_samples]\n",
    "x_values_features = features[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "x_train = K.cast_to_floatx(x_train)\n",
    "x_val = K.cast_to_floatx(x_val)\n",
    "\n",
    "x_train_features = K.cast_to_floatx(x_train_features)\n",
    "x_values_features = K.cast_to_floatx(x_values_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io, json\n",
    "import keras\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs)\n",
    "        with open(\"LTSM-2.txt\", \"w\") as text_file:\n",
    "            text = ''\n",
    "            for dicts in self.losses:\n",
    "                for key in dicts.keys():\n",
    "                    text += key + ':' + str(dicts[key]) + ', '\n",
    "                text += '\\n'\n",
    "            text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "left_branch = Sequential()\n",
    "left_branch.add(embedding_layer)\n",
    "left_branch.add(LSTM(100, return_sequences=False,activation = 'softsign'))\n",
    "left_branch.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(50, input_dim=number_of_features, activation='relu'))\n",
    "right_branch.add(Dense(20, activation='relu'))\n",
    "\n",
    "# Mege branchs\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "final_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callback = LossHistory()\n",
    "\n",
    "history = final_model.fit([x_train, x_train_features], \n",
    "                          y_train,callbacks=[callback],\n",
    "                          validation_data=([x_val, x_values_features],y_val),\n",
    "                          nb_epoch=20)  # we pass one data array per model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(history.history,os.path.join('../images','first_run' + '_100_history'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "for W_regularizer in [0, 0.25,0.5,0.75]:\n",
    "    for U_regularizer in [0, 0.25,0.5,0.75]:\n",
    "        for dropout_W in [0, 0.1,0.25]:\n",
    "            for dropout_U in [0,0.1,0.25]:\n",
    "                model2 = Sequential()\n",
    "                model2.add(embedding_layer)\n",
    "                model2.add(LSTM(100, return_sequences=False,\n",
    "                                     W_regularizer = W_regularizer,\n",
    "                                     U_regularizer = U_regularizer,\n",
    "                                     dropout_W = dropout_W,\n",
    "                                     dropout_U = dropout_U\n",
    "                               ))\n",
    "                model2.add(Dense(2, activation='sigmoid'))\n",
    "                model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                history = model2.fit(x_train, y_train, nb_epoch=50, batch_size=32,\n",
    "                                     validation_data=(x_val, y_val))\n",
    "                \n",
    "                params ='_'.join([str(W_regularizer),\n",
    "                                  str(U_regularizer),\n",
    "                                  str(dropout_W),\n",
    "                                  str(dropout_U)])\n",
    "                \n",
    "                joblib.dump(history.history,os.path.join('../images',params + '_100_history'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize = (8,8))\n",
    "ax.plot(history.history['loss'], label = 'loss')\n",
    "ax.plot(history.history['acc'], label = 'acc')\n",
    "ax.plot(history.history['val_acc'], label = 'val_acc')\n",
    "\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('LSTM - dropout_W = 0.1, dropout_U = 0.1')\n",
    "fig.savefig('results_multi_dropout_5_5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
