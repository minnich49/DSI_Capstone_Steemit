{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from DSI_Capstone_Steemit.utils.utils import(\n",
    "    load_data_and_description,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data,feature_names,data_desc = load_data_and_description(data_type='posts_tfidf')\n",
    "data_desc['log total_payout_value'] = np.log(data_desc['total_payout_value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = data_desc['log total_payout_value'] < 1.2\n",
    "idx2 = data_desc['log total_payout_value'] >2.5\n",
    "\n",
    "idx_not = (~idx1) & (~idx2)\n",
    "\n",
    "data_desc = data_desc[~idx_not]\n",
    "data = data[~idx_not.values,:]\n",
    "y = data_desc['log total_payout_value'] >2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = data_desc['category'].value_counts()\n",
    "\n",
    "top_categories = value_counts.index[value_counts > np.percentile(data_desc['category'].value_counts(),97)]\n",
    "\n",
    "idx = data_desc['category'].isin(top_categories)\n",
    "\n",
    "data_desc['top category'] = idx.astype(int)\n",
    "\n",
    "data_desc['top category listed'] = data_desc.ix[data_desc['top category'].values.astype(bool) ,'category']\n",
    "\n",
    "data_desc['top category listed'] = data_desc['top category listed'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ssp\n",
    "\n",
    "\n",
    "train_features = data_desc.ix[:,['number of body tags',\n",
    "                                   'number of body urls',\n",
    "                                   'number of image urls',\n",
    "                                   'number of body mentions',\n",
    "                                   'number of image urls',\n",
    "                                   'number of youtube urls',\n",
    "                                   'language',\n",
    "                                   'author_reputation_scaled',\n",
    "                                   'number of steem counts',\n",
    "                                'top category listed']]\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train_features)\n",
    "\n",
    "num_image_urls = train['number of image urls'].values[:,0]\n",
    "train.drop('number of image urls',axis = 1, inplace=True)\n",
    "\n",
    "train['number of image urls'] = num_image_urls\n",
    "\n",
    "training_names = train.columns\n",
    "\n",
    "train_sparse = ssp.csr_matrix(train)\n",
    "new_data = ssp.hstack([data,train_sparse])\n",
    "train = new_data.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(feature_names) + list(training_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_samples = 5000\n",
    "\n",
    "# All samples\n",
    "number_of_samples = train.shape[0]\n",
    "\n",
    "\n",
    "index = shuffle(range(train.shape[0]))[0:number_of_samples]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train[index], y[index], test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# y1_list = []\n",
    "# y2_list = []\n",
    "# lr_list = [0.001,0.005,0.01,0.05,0.1,1]\n",
    "# for lr in lr_list:\n",
    "#     eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "#     eval_metric = 'error'\n",
    "#     model = XGBClassifier(learning_rate=lr, n_estimators=1000)\n",
    "#     model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
    "\n",
    "#     results = model.evals_result()\n",
    "    \n",
    "#     y1 = results['validation_0'][eval_metric]\n",
    "#     y2 = results['validation_1'][eval_metric]\n",
    "\n",
    "#     y1_list.append(y1)\n",
    "#     y2_list.append(y2)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = len(results['validation_0'][eval_metric])\n",
    "# x_axis = range(0, epochs)\n",
    "# # plot log loss\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# colors = sns.color_palette()\n",
    "# for i,y1 in enumerate(y1_list):\n",
    "#     lr = lr_list[i]\n",
    "#     ax.plot(x_axis, y1, label='Train:' + str(lr), color = colors[i])\n",
    "#     ax.plot(x_axis, y2_list[i], label='Test:' + str(lr),color = colors[i])\n",
    "# ax.legend(loc = (1,0.5))\n",
    "# plt.ylabel('Error')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.title('XGBoost Error')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try and determine eta (learning rate) and number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (2 + 0.0) / 100, (10 + 0.0) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [1,3,5,7], \n",
    "             'min_child_weight': [1,3,7],\n",
    "             'colsample_bytree': [0.5,0.8,1],\n",
    "             'reg_alpha':[0.01,0.1,0.2],\n",
    "             'reg_lambda':[0.01,0.1,0.2]}\n",
    "\n",
    "\n",
    "# cv_params = {'max_depth': [1], \n",
    "#              'min_child_weight': [3],\n",
    "#              'colsample_bytree': [0.5]}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "lambda [default=1]\n",
    "L2 regularization term on weights (analogous to Ridge regression)\n",
    "This used to handle the regularization part of XGBoost. Though many data scientists donâ€™t use it often, it should be explored to reduce overfitting.\n",
    "alpha [default=0]\n",
    "L1 regularization term on weight (analogous to Lasso regression)\n",
    "Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "'''\n",
    "ind_params = {'learning_rate': 0.05, 'n_estimators': 100, 'seed':0, \n",
    "             'objective': 'binary:logistic'}\n",
    "model = xgb.XGBClassifier(**ind_params)\n",
    "optimized_GBM = GridSearchCV(model, \n",
    "                            cv_params, \n",
    "                             scoring ='accuracy', \n",
    "                             cv = 5, \n",
    "                             n_jobs = -1, \n",
    "                             verbose = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GBM.fit(X_train, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_crossval_results = joblib.load('grid_results/gridsearch_crossval_results')\n",
    "gridsearch_model = joblib.load('grid_results/gridsearch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_model = gridsearch_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_desc['log total_payout_value'] >2.5\n",
    "\n",
    "data_desc['true label'] = y\n",
    "\n",
    "desc_split_train,desc_split_test = train_test_split(\n",
    "    data_desc, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, y, test_size=0.33, random_state=42)\n",
    "\n",
    "xgb_best_params = gridsearch_model.best_params_\n",
    "\n",
    "best_estimator = xgb.XGBClassifier(n_estimators=500, \n",
    "                                   **xgb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "# kf.get_n_splits(train)\n",
    "\n",
    "# print(kf)  \n",
    "# probabilities_list = []\n",
    "# train_index_list = []\n",
    "# for train_index, test_index in kf.split(train):\n",
    "#     train_index_list.append(train_index)\n",
    "#     X_train, X_test = train[train_index], train[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     best_estimator.fit(X_train,y_train)\n",
    "    \n",
    "#     probabilities = best_estimator.predict_proba(X_test,y_test)\n",
    "#     probabilities_list.append(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = best_estimator.score(X_test,y_test) # Predict using our testdmat\n",
    "predictions = best_estimator.predict(X_test) # Predict using our testdmat\n",
    "probabilities = best_estimator.predict_proba(X_test)\n",
    "print y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_order = np.argsort(best_estimator.feature_importances_)[::-1]\n",
    "features_ordered = np.array(all_features)[sort_order]\n",
    "values_ordered = best_estimator.feature_importances_[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 50\n",
    "fig,ax = plt.subplots(1,1,figsize = (10,10))\n",
    "sns.barplot(x = values_ordered[0:top],y = features_ordered[0:top], ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print y_score,  y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_split_test['predictions'] = predictions\n",
    "desc_split_test['probability of 1'] = probabilities[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize = (10,5))\n",
    "\n",
    "sns.distplot(np.log(desc_split_test.ix[:,'total_payout_value']),ax =ax1, \n",
    "                 kde = False)\n",
    "\n",
    "# ax1.axvline(np.log(percentile_25))\n",
    "# ax1.axvline(np.log(percentile_50))\n",
    "# ax1.axvline(np.log(percentile_75))\n",
    "\n",
    "# ax1.text(x = np.log(percentile_25), y = 2000,s='25%',ha = 'center')\n",
    "# ax1.text(x = np.log(percentile_50), y = 2000,s='50%',ha = 'center')\n",
    "# ax1.text(x = np.log(percentile_75), y = 2000,s='60%',ha = 'center')\n",
    "ax1.set_title('Prediction Label Split',y = 1.1)\n",
    "ax1.set_xlabel('log(total_payout_value)')\n",
    "\n",
    "\n",
    "for label in np.sort(desc_split_test['predictions'].unique()):\n",
    "    idx = desc_split_test['predictions'] == label\n",
    "    sns.distplot(np.log(desc_split_test.ix[idx,'total_payout_value']),ax = ax2,\n",
    "                 kde = False, label = str(label))\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_title('Predictions of Gradient Boosting Model')\n",
    "ax2.set_xlabel('log(total_payout_value)')\n",
    "fig.tight_layout()\n",
    "plt.savefig('Distribution Plot - New.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_split_test['true label'] = (desc_split_test['total_payout_value'] > np.median(data_desc['total_payout_value'])).astype(int)\n",
    "# desc_split_test_short = desc_split_test.ix[:,['predictions','probability of 1','total_payout_value','true label']]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm =  confusion_matrix(y_pred= desc_split_test['predictions'], y_true=desc_split_test['true label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_normal = (cm + 0.0)/desc_split_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm,cmap='Blues', annot=True,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4],\n",
    "            fmt='d')\n",
    "plt.savefig('Confusion Matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm_normal,cmap='Blues', annot=True,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4],\n",
    "            fmt='f')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Confusion Matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_split_test['Difference'] = abs(desc_split_test['predictions'] - desc_split_test['true label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_split_test['length of article'] = desc_split_test['body'].fillna('').str.split(' ').apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log(desc_split_test['length of article']))\n",
    "plt.axvline(np.log(desc_split_test['length of article'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_split_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_split_test['author_reputation_scaled'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_split_test['Log Difference'] = np.log(desc_split_test['length of article'].values )\n",
    "desc_split_test.ix[desc_split_test['Difference'] > 2,['Log Difference','true label',\n",
    "                                                      'predictions',\n",
    "                                                      'Difference',\n",
    "                                                      'length of article',\n",
    "                                                      'number of body mentions',\n",
    "                                                      'number of image urls',\n",
    "                                                     'whale', u'body whale mentions',\n",
    "                                                     'author_reputation_scaled']].sort_values('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'length of article',y = 'total_payout_value',data = desc_split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
