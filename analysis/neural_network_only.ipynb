{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "from DSI_Capstone_Steemit.utils.utils import(\n",
    "    load_data_and_description,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "data_directory = '../data/'\n",
    "input_directory = os.path.join(data_directory,'networkx_votes')\n",
    "\n",
    "def load_joblib(filename):\n",
    "    return joblib.load(os.path.join(input_directory,filename))\n",
    "\n",
    "data,feature_names,data_desc = load_data_and_description(data_type='tfidf')\n",
    "data_desc['log total_payout_value'] = np.log(data_desc['total_payout_value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_network_features(df):\n",
    "\n",
    "    hubs,authorities = load_joblib('hits')\n",
    "    cluster = load_joblib('parts')\n",
    "    pagerank = load_joblib('prank') \n",
    "    eig_cent = load_joblib('eig_cent') \n",
    "    core_k = load_joblib('core_k') \n",
    "\n",
    "    df['Cluster'] = df['author'].map(cluster)\n",
    "    df.loc[:,'Cluster Condense'] = df['Cluster']\n",
    "    df.loc[~df['Cluster'].isin([1,3,0,2,5,4]),'Cluster Condense'] = 'Other'\n",
    "    df['Hubs'] = df['author'].map(hubs) * 10000\n",
    "    df['Authorities'] = df['author'].map(authorities) * 10000\n",
    "    df['Page Rank'] = df['author'].map(pagerank) * 10000\n",
    "    df['Eigen Centrality'] = df['author'].map(eig_cent)* 10000\n",
    "    df['Core K'] = df['author'].map(core_k)*10000\n",
    "    return df\n",
    "data_desc = add_network_features(data_desc)\n",
    "\n",
    "network_cols = ['Page Rank','Cluster','Hubs','Authorities','Page Rank','Eigen Centrality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove middle value articles\n",
    "\n",
    "idx1 = data_desc['log total_payout_value'] < 1.2\n",
    "idx2 = data_desc['log total_payout_value'] >2.5\n",
    "\n",
    "idx_not = (~idx1) & (~idx2)\n",
    "\n",
    "data_desc = data_desc[~idx_not]\n",
    "data = data[~idx_not.values,:]\n",
    "y = data_desc['log total_payout_value'] >2.5\n",
    "\n",
    "# For Regression\n",
    "# y = data_desc['log total_payout_value']\n",
    "value_counts = data_desc['category'].value_counts()\n",
    "top_categories = value_counts.index[value_counts > np.percentile(data_desc['category'].value_counts(),97)]\n",
    "idx = data_desc['category'].isin(top_categories)\n",
    "data_desc['top category'] = idx.astype(int)\n",
    "\n",
    "data_desc['top category listed'] = data_desc.ix[data_desc['top category'].values.astype(bool) ,'category']\n",
    "\n",
    "data_desc['top category listed'] = data_desc['top category listed'].fillna('Other')\n",
    "\n",
    "\n",
    "post_features = ['number of body tags',\n",
    "                                   'number of body urls',\n",
    "                                   'number of image urls',\n",
    "                                   'number of body mentions',\n",
    "                                   'number of image urls',\n",
    "                                   'number of youtube urls',\n",
    "                                   'language',\n",
    "                                   'author_reputation_scaled',\n",
    "                                   'number of steem counts',\n",
    "                                'top category'] + network_cols\n",
    "\n",
    "\n",
    "train_features = data_desc.ix[:,post_features].fillna(0)\n",
    "\n",
    "train = pd.get_dummies(train_features,columns=['language','Cluster'])\n",
    "\n",
    "num_image_urls = train['number of image urls'].values[:,0]\n",
    "train.drop('number of image urls',axis = 1, inplace=True)\n",
    "\n",
    "train['number of image urls'] = num_image_urls\n",
    "\n",
    "training_names = train.columns\n",
    "\n",
    "train_sparse = ssp.csr_matrix(train)\n",
    "new_data = ssp.hstack([data,train_sparse])\n",
    "train = new_data.tocsr()\n",
    "\n",
    "# All samples\n",
    "number_of_samples = train.shape[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = X_train\n",
    "X = X_train.toarray()#[0:samples]\n",
    "y = y_train.astype(int).values#[0:samples]\n",
    "# y = keras.utils.np_utils.to_categorical(y)\n",
    "\n",
    "\n",
    "\n",
    "X_val = X_test.toarray()#[0:samples]\n",
    "\n",
    "y_val = y_test.astype(int).values#[0:samples]\n",
    "# y_val = keras.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_val).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_values = f_classif(X,y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,pvalues = f_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (pvalues < 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.array(feature_names + list(training_names))\n",
    "all_features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val[:,idx]\n",
    "X = X[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X.shape\n",
    "print X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import log_loss, roc_auc_score,accuracy_score\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta, Adam, rmsprop\n",
    "\n",
    "\n",
    "space = {\n",
    "        'lr': hp.uniform('lr', 0.0001,0.1),\n",
    "}\n",
    "\n",
    "dropout = 0.5\n",
    "def f_nn(params):   \n",
    "\n",
    "    print ('Params testing: ', params)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=1000, input_dim = X.shape[1])) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(output_dim=500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(output_dim=100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(output_dim=10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adagrad(params['lr']))\n",
    "\n",
    "    model.fit(X, y, nb_epoch=10, batch_size=32, verbose = 1)\n",
    "\n",
    "    pred_auc =model.predict_classes(X_val, batch_size = 32, verbose = 0)\n",
    "    acc = accuracy_score(y_val, pred_auc)\n",
    "    \n",
    "    print('Accuracy:', acc)\n",
    "    sys.stdout.flush() \n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, space, algo=tpe.suggest, max_evals=25, trials=trials)\n",
    "print 'best: '\n",
    "print best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=500, input_dim = X.shape[1])) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "model.add(Dense(output_dim=250))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adagrad(0.034118198130292786))\n",
    "\n",
    "model.fit(X, y, nb_epoch=3, batch_size=32, verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
