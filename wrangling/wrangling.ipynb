{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_directory = os.path.join(os.getcwd().split('DSI_Capstone_Steemit')[0])\n",
    "sys.path.insert(1,module_directory)\n",
    "\n",
    "from DSI_Capstone_Steemit.utils.utils import(\n",
    "    ensure_directories\n",
    ")\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "dir_list = ['posts_tfidf',\n",
    "            'posts_counts',\n",
    "            'word2vec_doc_matrix_avg',\n",
    "            'word2vec_doc_matrix_avg_tfidf',\n",
    "           'posts_raw_cleaned']\n",
    "ensure_directories(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_directory = os.path.join(os.getcwd().split('DSI_Capstone_Steemit')[0])\n",
    "sys.path.insert(1,module_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DSI_Capstone_Steemit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymssql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "data_directory = '../data/'\n",
    "\n",
    "posts_path = os.path.join(data_directory,'sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_csv(posts_path)\n",
    "# Remove all unicode related characters\n",
    "df_posts['body'] = df_posts['body'].str.decode('unicode_escape').str.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple updates to articles to get one body per post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_body = df_posts.groupby(['author','permlink']).agg(lambda x: ''.join(set(x))).reset_index()\n",
    "combined_bFody = combined_body.ix[:,['body','author','permlink']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates, due to multiple updates, then combine with full body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "idx_not_duplicates = ~df_posts.duplicated(['author','permlink'])\n",
    "df_posts = df_posts.ix[idx_not_duplicates,:]\n",
    "df_posts.drop('body',axis = 1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_posts = pd.merge(df_posts,combined_body,on=['author','permlink'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.sort_values(by='total_payout_value',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = r'http\\S+'\n",
    "\n",
    "# Extract all Links\n",
    "df_posts['body urls'] = df_posts['body'].str.findall(expression)\n",
    "df_posts['number of body urls'] = df_posts['body urls'].apply(len)\n",
    "df_posts['number of youtube urls'] = (df_posts.ix[:,'body urls']\n",
    "                                      .str.join(' ')\n",
    "                                      .str.replace('\\.','')\n",
    "                                      .str.count('youtube'))\n",
    "\n",
    "df_posts['number of image urls'] = (df_posts.ix[:,'body urls']\n",
    "                                    .str.join(' ')\n",
    "                                    .str.count('jpg|png|gif|jpeg'))\n",
    "\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "# Remove Hashtags\n",
    "expression = '#(\\S+)'\n",
    "# Extract all Hash Tages\n",
    "df_posts['body tags'] = df_posts['body'].str.findall(expression)\n",
    "df_posts['number of body tags'] = df_posts['body tags'].apply(len)\n",
    "\n",
    "# Remove all Tags\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "\n",
    "# Remove Mentions : @thecryptodrive\n",
    "expression = '@(\\S+)'\n",
    "# Extract all Hash Tages\n",
    "df_posts['body mentions'] = df_posts['body'].str.findall(expression)\n",
    "df_posts['number of body mentions'] = df_posts['body mentions'].apply(len)\n",
    "\n",
    "# Remove all Tags\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "\n",
    "\n",
    "# Remove all Links\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "\n",
    "# Remove all periods\n",
    "expression = '\\.'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,' ')\n",
    "\n",
    "# # Remove all new lines\n",
    "expression = r'\\n'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,' ')\n",
    "\n",
    "\n",
    "# These can probably be removed\n",
    "# # Extract/Remove Markdown Related for Headers\n",
    "# expression = '0A0A(.*?)0A0A'\n",
    "# df_posts['body headers'] = df_posts['body'].str.findall(expression)\n",
    "\n",
    "\n",
    "# # Extract/Remove Markdown Related for Code\n",
    "# expression = '60(.*?)60'\n",
    "# df_posts['body code'] = df_posts['body'].str.findall(expression)\n",
    "\n",
    "# expression = '\\d+'\n",
    "# df_posts['body'] = df_posts['body'].str.replace(expression,' ')\n",
    "\n",
    "\n",
    "\n",
    "# Remove Any Capital Letter by themselves A, B, C, D etc\n",
    "expression = r'\\b[A-Z]\\b'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "\n",
    "# Remove double spaces\n",
    "expression = ' +'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,' ')\n",
    "\n",
    "# Remove pure numerical values that have greater than 5 digits\n",
    "expression = r'\\b[0-9]{5,100}\\b'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "\n",
    "# Remove all non alpha numeric\n",
    "expression = '[^A-Za-z0-9 ]+'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,'')\n",
    "\n",
    "expression = '0A0A'\n",
    "df_posts['body'] = df_posts['body'].str.replace(expression,' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_posts['body'] = df_posts['body'].str.decode('unicode_escape').str.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.ix[0,'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_raw_cleaned = os.path.join(data_directory,\n",
    "                                             'posts_raw_cleaned', \n",
    "                                             'posts_raw_cleaned.csv')\n",
    "\n",
    "\n",
    "df_posts.to_csv(posts_raw_cleaned,\n",
    "                              index=False, \n",
    "                              quoting=csv.QUOTE_ALL, \n",
    "                              encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save Word Counts, TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "porter =  PorterStemmer()\n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = porter.stem\n",
    "    def __call__(self, doc):\n",
    "        return [self.porter(t) for t in word_tokenize(doc)]\n",
    "\n",
    "countvect = CountVectorizer(\n",
    "    encoding = 'utf-8',\n",
    "    tokenizer = PorterTokenizer(),\n",
    "    stop_words = stopwords.words('english'),\n",
    "    lowercase = False\n",
    "    \n",
    ")\n",
    "\n",
    "tfidfvect = TfidfVectorizer(\n",
    "    encoding = 'utf-8',\n",
    "    tokenizer = PorterTokenizer(),\n",
    "    stop_words = stopwords.words('english'),\n",
    "    lowercase = False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_counts = countvect.fit_transform(df_posts['body'])\n",
    "posts_tfidf = tfidfvect.fit_transform(df_posts['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_counts_path = os.path.join(data_directory,'posts_counts', 'posts_counts')\n",
    "posts_tfidf_path = os.path.join(data_directory,'posts_tfidf', 'posts_tfidf')\n",
    "\n",
    "joblib.dump(posts_counts,posts_counts_path)\n",
    "joblib.dump(countvect.get_feature_names(),posts_counts_path+'_feature_names')\n",
    "\n",
    "joblib.dump(posts_tfidf,posts_tfidf_path)\n",
    "joblib.dump(tfidfvect.get_feature_names(),posts_tfidf_path+'_feature_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_posts.shape, posts_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data that goes with counts and vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.drop('body',axis = 1,inplace=True)\n",
    "\n",
    "posts_counts_desc_path = os.path.join(data_directory,\n",
    "                                             'posts_counts', \n",
    "                                             'posts_counts_desc.csv')\n",
    "\n",
    "\n",
    "df_posts.to_csv(posts_counts_desc_path,\n",
    "                              index=False, \n",
    "                              quoting=csv.QUOTE_ALL, \n",
    "                              encoding='utf-8')\n",
    "\n",
    "posts_tfidf_desc_path = os.path.join(data_directory,\n",
    "                                             'posts_tfidf', \n",
    "                                             'posts_tfidf_desc.csv')\n",
    "\n",
    "\n",
    "df_posts.to_csv(posts_tfidf_desc_path,\n",
    "                              index=False, \n",
    "                              quoting=csv.QUOTE_ALL, \n",
    "                              encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
